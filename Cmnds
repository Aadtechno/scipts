gsutil ls -l -r gs://your-bucket-name/ | awk '$1 ~ /^[0-9]/ && $2 < "2024-12-01" {print $1, $2, $3; total+=$1} END {print "Total Size (bytes):", total}'

nohup sh -c 'gsutil ls -l -r gs://your-bucket-name/ | awk '"'"'$1 ~ /^[0-9]/ && $2 < "2024-12-01" {total+=$1} END {cmd="numfmt --to=iec-i --suffix=B"; print total | cmd}'"'"' > bucket_size_before_dec2024.txt 2>&1 &'

gsutil ls -l -r gs://your-bucket-name/ | awk '$1 ~ /^[0-9]/ && $2 < "2024-12-01" {count++} END {print "Total files before December 2024:", count}'


gcloud storage objects list --bucket your-bucket-name --filter="timeCreated>=2022-01-01T00:00:00Z AND timeCreated<2023-01-01T00:00:00Z" --format="value(name)"


gsutil ls -l gs://your-bucket-name/** | awk '$2 >= "2022-01-01T00:00:00Z" && $2 < "2023-01-01T00:00:00Z" {print $3}'


gsutil ls -l gs://your-bucket-name/** --versions | awk '$2 >= "2022-01-01T00:00:00Z" && $2 < "2023-01-01T00:00:00Z" {print $3}'

___
gsutil ls -l gs://your-bucket-name/** --versions | awk '$2 >= "2022-01-01T00:00:00Z" && $2 < "2023-01-01T00:00:00Z" {print $3}'


gsutil ls -l gs://your-bucket-name/** --versions | awk '$2 >= "2022-01-01T00:00:00Z" && $2 < "2023-01-01T00:00:00Z" {print $1}' | xargs -I {} gsutil -m rm {}

gcloud storage objects list --bucket gs://your-bucket-name --filter "timeCreated >= '2024-01-01T00:00:00Z' AND timeCreated < '2024-02-01T00:00:00Z'" --format="value(name,timeCreated)" | grep '\.avro$'

gsutil ls -l gs://your-bucket-name/** | awk '$2 >= "2024-01-01T00:00:00Z" && $2 < "2024-02-01T00:00:00Z" && $3 ~ /\.avro$/ {print $3, $2}'

gcloud storage objects list --bucket gs://your-bucket-name --format="value(name,timeCreated)" | grep '\.avro$' | awk '$2 >= "2024-01-01T00:00:00Z" && $2 < "2024-02-01T00:00:00Z" {print $1, $2}'

pipeline {
    agent any  // You can specify a node label if needed

    parameters {
        string(name: 'GCS_BUCKET', defaultValue: 'your-bucket-name', description: 'Google Cloud Storage Bucket Name')
        string(name: 'GCS_FILE_PATH', defaultValue: 'folder/file_to_delete.txt', description: 'Path to the file that contains the GCS paths to delete')
    }

    stages {
        stage('Download File from GCS') {
            steps {
                script {
                    echo "Downloading file from GCS: gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH}"
                    // Download the file from GCS to the local workspace
                    sh "gsutil cp gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH} file_to_delete.txt"
                }
            }
        }

        stage('Delete Files from GCS') {
            steps {
                script {
                    echo "Deleting files from GCS based on file content"
                    // Delete files in parallel using xargs
                    sh '''
                    cat file_to_delete.txt | xargs -I {} -P 10 gcloud storage rm {}
                    '''
                }
            }
        }
    }

    post {
        success {
            echo "Successfully deleted files."
        }
        failure {
            echo "File deletion failed."
        }
    }
}
-----
pipeline {
    agent any  // You can specify a node label if needed

    parameters {
        string(name: 'GCS_BUCKET', defaultValue: 'your-bucket-name', description: 'Google Cloud Storage Bucket Name')
        string(name: 'GCS_FILE_PATH', defaultValue: 'folder/file_to_delete.txt', description: 'Path to the file that contains the GCS paths to delete')
    }

    environment {
        GOOGLE_APPLICATION_CREDENTIALS = '/path/to/your/service-account-key.json'  // If you need credentials for GCS
    }

    stages {
        stage('Download File from GCS') {
            steps {
                script {
                    echo "Downloading file from GCS: gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH}"
                    // Download the file from GCS to Jenkins workspace
                    sh "gsutil cp gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH} file_to_delete.txt"
                }
            }
        }

        stage('Split File into Chunks') {
            steps {
                script {
                    echo "Splitting the file into smaller chunks"
                    // Split the file into smaller chunks of 1 million lines
                    sh 'split -l 1000 file_to_delete.txt chunk_'
                }
            }
        }

        stage('Delete Files in Parallel') {
            steps {
                script {
                    echo "Deleting files from GCS in parallel based on file chunks"
                    // Use parallel to process each chunk and delete files in parallel
                    sh '''
                    for chunk in chunk_*; do
                        cat $chunk | parallel -j 50 gcloud storage rm {}
                    done
                    '''
                }
            }
        }
    }
}
-----
pipeline {
    agent any  // You can specify a node label if needed

    parameters {
        string(name: 'GCS_BUCKET', defaultValue: 'your-bucket-name', description: 'Google Cloud Storage Bucket Name')
        string(name: 'GCS_FILE_PATH', defaultValue: 'folder/file_to_delete.txt', description: 'Path to the file that contains the GCS paths to delete')
    }

    environment {
        GOOGLE_APPLICATION_CREDENTIALS = '/path/to/your/service-account-key.json'  // If you need credentials for GCS
    }

    stages {
        stage('Download File from GCS') {
            steps {
                script {
                    echo "Downloading file from GCS: gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH}"
                    // Download the file from GCS to Jenkins workspace using the GCS_FILE_PATH parameter
                    sh "gsutil cp gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH} ${params.GCS_FILE_PATH}"
                }
            }
        }

        stage('Split File into Chunks') {
            steps {
                script {
                    echo "Splitting the file into smaller chunks"
                    // Split the file into smaller chunks of 1 million lines
                    sh """
                    split -l 1000000 ${params.GCS_FILE_PATH} ${params.GCS_FILE_PATH}_chunk_
                    """
                }
            }
        }

        stage('Delete Files in Parallel') {
            steps {
                script {
                    echo "Deleting files from GCS in parallel based on file chunks"

                    // Loop through each chunk and delete files using xargs
                    sh '''
                    for chunk in ${params.GCS_FILE_PATH}_chunk_*; do
                        if [ -f "$chunk" ]; then
                            echo "Deleting chunk: $chunk"
                            cat $chunk | xargs -n 1000 -P 50 gcloud storage rm "{}"
                        else
                            echo "Chunk file not found, skipping: $chunk"
                        fi
                    done
                    '''
                }
            }
        }
    }
}
-----
pipeline {
    agent any  // You can specify a node label if needed

    parameters {
        string(name: 'GCS_BUCKET', defaultValue: 'your-bucket-name', description: 'Google Cloud Storage Bucket Name')
        string(name: 'GCS_FILE_PATH', defaultValue: 'folder/file_to_delete.txt', description: 'Path to the file that contains the GCS paths to delete')
    }

    environment {
        GOOGLE_APPLICATION_CREDENTIALS = '/path/to/your/service-account-key.json'  // If you need credentials for GCS
    }

    stages {
        stage('Download File from GCS') {
            steps {
                script {
                    echo "Downloading file from GCS: gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH}"
                    // Use the parameter value GCS_FILE_PATH for the local file name
                    sh "gsutil cp gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH} ${params.GCS_FILE_PATH}"
                }
            }
        }

        stage('Split File into Chunks') {
            steps {
                script {
                    echo "Splitting the file into smaller chunks"
                    // Extract the base name and use it for chunk files
                    def baseName = params.GCS_FILE_PATH.split('/').last().replace('.txt', '')
                    echo "Base name for chunking: ${baseName}"

                    // Split the file into smaller chunks of 1 million lines
                    // Output chunk files will be named based on the base name extracted from the path
                    sh """
                    split -l 1000000 ${params.GCS_FILE_PATH} ${baseName}_chunk_
                    """
                }
            }
        }

        stage('Delete Files in Parallel') {
            steps {
                script {
                    echo "Deleting files from GCS in parallel based on file chunks"
                    // Use parallel to process each chunk and delete files in parallel
                    sh '''
                    for chunk in ${baseName}_chunk_*; do
                        cat $chunk | parallel -j 50 gcloud storage rm {}
                    done
                    '''
                }
            }
        }
    }
}
-----
pipeline {
    agent any  // You can specify a node label if needed

    parameters {
        string(name: 'GCS_BUCKET', defaultValue: 'your-bucket-name', description: 'Google Cloud Storage Bucket Name')
        string(name: 'GCS_FILE_PATH', defaultValue: 'folder/file_to_delete.txt', description: 'Path to the file that contains the GCS paths to delete')
    }

    environment {
        GOOGLE_APPLICATION_CREDENTIALS = '/path/to/your/service-account-key.json'  // If you need credentials for GCS
    }

    stages {
        stage('Download File from GCS') {
            steps {
                script {
                    echo "Downloading file from GCS: gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH}"
                    // Use the parameter value GCS_FILE_PATH for the local file name
                    sh "gsutil cp gs://${params.GCS_BUCKET}/${params.GCS_FILE_PATH} ${params.GCS_FILE_PATH}"
                }
            }
        }

        stage('Split File into Chunks') {
            steps {
                script {
                    echo "Splitting the file into smaller chunks"

                    // Extract the base name and use it for chunk files
                    def baseName = params.GCS_FILE_PATH.split('/').last().replace('.txt', '')
                    echo "Base name for chunking: ${baseName}"

                    // Split the file into smaller chunks of 1 million lines
                    // Output chunk files will be named based on the base name extracted from the path
                    sh """
                    split -l 1000000 ${params.GCS_FILE_PATH} ${baseName}_chunk_
                    """
                }
            }
        }

        stage('Delete Files in Parallel') {
            steps {
                script {
                    echo "Deleting files from GCS in parallel based on file chunks"

                    // Get a list of chunk files and process them in parallel using xargs
                    sh '''
                    for chunk in ${baseName}_chunk_*; do
                        if [ -f "$chunk" ]; then
                            echo "Deleting chunk: $chunk"
                            cat "$chunk" | xargs -n 1000 -P 50 gcloud storage rm "{}" || echo "Error deleting chunk: $chunk"
                        else
                            echo "Chunk file not found, skipping: $chunk"
                        fi
                    done
                    '''
                }
            }
        }
    }
}


